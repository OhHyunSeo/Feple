{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d823bbf0-2ae3-4dbb-8ef4-05058ca00399",
   "metadata": {},
   "source": [
    "## 1. 압축 파일 압축 해제 코드 (압축 폴더가 있을시)\n",
    "### 라벨링데이터.zip -> data 폴더에 해제\n",
    "#### data_v3 폴더 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3358522b-e745-40f4-ac7d-c1542dba4d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unzipping files: 100%|██████████████████| 35270/35270 [00:03<00:00, 9063.90it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "zip_path   = '라벨링데이터.zip'\n",
    "extract_to = 'data'\n",
    "\n",
    "# 출력 폴더 생성\n",
    "os.makedirs(extract_to, exist_ok=True)\n",
    "\n",
    "def decode_filename(fname: str) -> str:\n",
    "    \"\"\"\n",
    "    zipfile이 cp437로 디코딩한 fname(garbled)을\n",
    "    1) cp437 바이트로 다시 인코딩 → UTF-8 디코딩 시도\n",
    "    2) 실패 시 cp949 디코딩 시도\n",
    "    3) 그마저 실패하면 원본 fname 반환\n",
    "    \"\"\"\n",
    "    raw = fname.encode('cp437', errors='ignore')\n",
    "    for enc in ('utf-8', 'cp949'):\n",
    "        try:\n",
    "            return raw.decode(enc)\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    return fname\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "    # __MACOSX, .DS_Store, '._'로 시작하는 메타데이터는 제외\n",
    "    entries = [\n",
    "        info for info in z.infolist()\n",
    "        if not (\n",
    "            info.filename.startswith('__MACOSX/') or\n",
    "            os.path.basename(info.filename).startswith('._') or\n",
    "            os.path.basename(info.filename) == '.DS_Store'\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    for info in tqdm(entries, desc='Unzipping files'):\n",
    "        # 파일명을 깨짐 없이 복원\n",
    "        safe_name = decode_filename(info.filename)\n",
    "        out_path   = os.path.join(extract_to, safe_name)\n",
    "\n",
    "        if info.is_dir():\n",
    "            os.makedirs(out_path, exist_ok=True)\n",
    "        else:\n",
    "            os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "            with z.open(info) as src, open(out_path, 'wb') as dst:\n",
    "                shutil.copyfileobj(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89c3104e-1f3a-4d39-bfd8-c10c873cf8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging classification 40001: 100%|█████████████| 5/5 [00:00<00:00, 2931.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 5개 파일을 병합하여 output_v3/merged_classification_40001.json에 저장했습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 테스트 코드\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1) 처리할 세션 ID 지정\n",
    "session_id = '40001'\n",
    "\n",
    "# 2) 분류 폴더 경로, 파일 패턴\n",
    "input_dir   = 'data_v3/classification'\n",
    "pattern     = os.path.join(input_dir, f'*_{session_id}_*.json')\n",
    "\n",
    "# 3) 파일 리스트 정렬 (1→2→3→4→5.json 순)\n",
    "files = sorted(\n",
    "    glob.glob(pattern),\n",
    "    key=lambda p: int(os.path.basename(p).split('_')[-1].split('.')[0])\n",
    ")\n",
    "\n",
    "# 4) 진행바로 5개 파일 로드\n",
    "merged_list = []\n",
    "for path in tqdm(files, desc=f'Merging classification {session_id}'):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        merged_list.append(json.load(f))\n",
    "\n",
    "# 5) 하나의 객체로 합치기\n",
    "out_obj = {\n",
    "    'session_id': session_id,\n",
    "    '분류': merged_list    # 5개의 JSON 내용이 리스트로\n",
    "}\n",
    "\n",
    "# 6) output 폴더에 저장\n",
    "os.makedirs('output_v3', exist_ok=True)\n",
    "out_path = os.path.join('output_v3', f'merged_classification_{session_id}.json')\n",
    "with open(out_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(out_obj, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f'✅ {len(files)}개 파일을 병합하여 {out_path}에 저장했습니다.')  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849393ae-44ef-4950-99a8-e3e9d3357e6d",
   "metadata": {},
   "source": [
    "#### 중복 내용 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "381f9c04-abd2-498b-bb92-0cfaf17453e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging duplicates: 100%|██████████████████████| 5/5 [00:00<00:00, 61500.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 중복 제거 및 병합 완료: 5 → 1 items\n",
      "생성된 파일: output_v3/merged_classification_40001_cleaned.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1) 입력·출력 경로\n",
    "input_path  = 'output_v3/merged_classification_40001.json'\n",
    "output_path = 'output_v3/merged_classification_40001_cleaned.json'\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "# 2) 원본 로드\n",
    "with open(input_path, 'r', encoding='utf-8') as f:\n",
    "    merged = json.load(f)\n",
    "\n",
    "# 3) 콘텐츠별로 묶어서 instructions 합치기\n",
    "grouped = {}  # content_str → {'base': record_without_instr, 'data': [instr_data, ...]}\n",
    "\n",
    "for entry in tqdm(merged['분류'], desc='Merging duplicates'):\n",
    "    # entry가 list 안에 dict 하나로 들어 있으면 꺼내고, 아니면 그대로\n",
    "    rec = entry[0] if isinstance(entry, list) else entry\n",
    "\n",
    "    content = rec['consulting_content']\n",
    "    # instructions 구조 안쪽의 data 리스트\n",
    "    instr_block = rec['instructions'][0]            # e.g. {'tuning_type':'분류','data':[...]}\n",
    "    data_list   = instr_block.get('data', [])\n",
    "\n",
    "    if content not in grouped:\n",
    "        # base에는 중복시켜 쓸 필드들만 남겨 둠\n",
    "        base = {k: v for k, v in rec.items() if k != 'instructions'}\n",
    "        grouped[content] = {'base': base, 'data': []}\n",
    "\n",
    "    # 같은 content에 대한 모든 data_list를 합침\n",
    "    grouped[content]['data'].extend(data_list)\n",
    "\n",
    "# 4) 최종 리스트 재구성\n",
    "cleaned = []\n",
    "for content, info in grouped.items():\n",
    "    obj = info['base'].copy()\n",
    "    # instructions 자리에 합쳐진 data만 넣어서 하나의 튜닝블록으로 만듦\n",
    "    obj['instructions'] = [{\n",
    "        'tuning_type': '분류',\n",
    "        'data': info['data']\n",
    "    }]\n",
    "    cleaned.append(obj)\n",
    "\n",
    "# 5) 저장\n",
    "out = {\n",
    "    'session_id': merged.get('session_id'),\n",
    "    '분류': cleaned\n",
    "}\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(out, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f'✅ 중복 제거 및 병합 완료: {len(merged[\"분류\"])} → {len(cleaned)} items')\n",
    "print('생성된 파일:', output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f05de6c-22f1-42d0-a85f-633e7d2afabe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deduping instruction inputs: 100%|██████████████| 1/1 [00:00<00:00, 9822.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 완료: 중복된 input 제거 후 저장 → output/merged_classification_40001_cleaned_once.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1) 입력·출력 경로\n",
    "input_path  = 'output/merged_classification_40001_cleaned.json'\n",
    "output_path = 'output/merged_classification_40001_cleaned_once.json'\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "# 2) 로드\n",
    "with open(input_path, 'r', encoding='utf-8') as f:\n",
    "    merged = json.load(f)\n",
    "\n",
    "# 3) 각 레코드별로 중복된 'input' 제거하고, 최상단으로 한 번만 옮기기\n",
    "for rec in tqdm(merged['분류'], desc='Deduping instruction inputs'):\n",
    "    data_list = rec['instructions'][0]['data']\n",
    "    if not data_list:\n",
    "        continue\n",
    "    # 3-1) 공통 input 뽑아서 최상단에 추가\n",
    "    common_input = data_list[0].get('input', '')\n",
    "    rec['input'] = common_input\n",
    "    \n",
    "    # 3-2) 각 data 항목에서 중복된 input 키 제거\n",
    "    for d in data_list:\n",
    "        if 'input' in d:\n",
    "            d.pop('input')\n",
    "\n",
    "# 4) 저장\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(merged, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f'✅ 완료: 중복된 input 제거 후 저장 → {output_path}')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16dc8965-dbf3-4430-b950-b89f052857cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Removing top-level input: 100%|████████████████| 1/1 [00:00<00:00, 12157.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 완료: 최종 파일 → output/merged_classification_40001_final.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1) 입력·출력 파일 경로\n",
    "input_path  = 'output/merged_classification_40001_cleaned_once.json'\n",
    "output_path = 'output/merged_classification_40001_final.json'\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "# 2) JSON 로드\n",
    "with open(input_path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 3) '분류' 리스트의 각 레코드에서 'input' 키 제거\n",
    "for rec in tqdm(data.get('분류', []), desc='Removing top-level input'):\n",
    "    rec.pop('input', None)\n",
    "\n",
    "# 4) 결과 저장\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f'✅ 완료: 최종 파일 → {output_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf911eb8-7bde-44a9-8270-5c1be19ca02e",
   "metadata": {},
   "source": [
    "## 진짜 시작 - 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6c40a7f3-9763-473a-91cd-6e038eb1b5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "세션 처리: 100%|███████████████████████████| 3600/3600 [00:03<00:00, 968.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 모든 세션 병합 완료!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 입력·출력 폴더 설정\n",
    "INPUT_DIR  = 'data_v3/classification'\n",
    "OUTPUT_DIR = 'json_merge/classification_merge_output_v3'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# 1) 파일명 언더스코어 분할로 세션ID별 그룹화\n",
    "sessions = defaultdict(list)\n",
    "for fp in glob.glob(os.path.join(INPUT_DIR, '*.json')):\n",
    "    fname = os.path.basename(fp)\n",
    "    parts = fname.split('_')\n",
    "    # parts = ['01', '분류', '40017', '3.json']\n",
    "    if len(parts) >= 4:\n",
    "        session_id = parts[2]\n",
    "        sessions[session_id].append(fp)\n",
    "\n",
    "# 2) 각 세션별로 5개 파일만 처리\n",
    "for session_id, fps in tqdm(sessions.items(), desc='세션 처리'):\n",
    "    if len(fps) != 5:\n",
    "        continue\n",
    "\n",
    "    # 3) 번호 순서대로 정렬(1.json → 2.json … 5.json)\n",
    "    ordered = sorted(\n",
    "        fps,\n",
    "        key=lambda p: int(os.path.basename(p).split('_')[-1].split('.')[0])\n",
    "    )\n",
    "\n",
    "    # 4) JSON 로드 & consulting_content 기준으로 data 병합\n",
    "    merged = {}\n",
    "    for p in ordered:\n",
    "        arr = json.load(open(p, 'r', encoding='utf-8'))\n",
    "        rec = arr[0] if isinstance(arr, list) else arr\n",
    "        content = rec['consulting_content']\n",
    "\n",
    "        if content not in merged:\n",
    "            # base 메타( instructions, input 제외 )\n",
    "            base = {k: v for k, v in rec.items() if k not in ('instructions', 'input')}\n",
    "            merged[content] = {'base': base, 'data': []}\n",
    "\n",
    "        # instructions[0]['data']만 합치기\n",
    "        merged[content]['data'].extend(\n",
    "            rec['instructions'][0].get('data', [])\n",
    "        )\n",
    "\n",
    "    # 5) 최종 오브젝트 리스트 생성\n",
    "    cleaned = []\n",
    "    for content, info in merged.items():\n",
    "        obj = info['base'].copy()\n",
    "        obj['consulting_content'] = content\n",
    "        obj['instructions'] = [{\n",
    "            'tuning_type': '분류',\n",
    "            'data': info['data']\n",
    "        }]\n",
    "        cleaned.append(obj)\n",
    "\n",
    "    # 6) 세션별로 파일 저장\n",
    "    out = {\n",
    "        'session_id': session_id,\n",
    "        '분류': cleaned\n",
    "    }\n",
    "    out_path = os.path.join(OUTPUT_DIR, f'merged_classification_{session_id}_final.json')\n",
    "    with open(out_path, 'w', encoding='utf-8') as fw:\n",
    "        json.dump(out, fw, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"✅ 모든 세션 병합 완료!\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d7aaa1bb-b162-41b3-b219-b57a6fee796d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Removing duplicate inputs: 100%|██████████| 3600/3600 [00:00<00:00, 4235.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Removed \"input\" fields from 3600 merged JSON files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1) 출력 폴더와 파일 패턴 정의\n",
    "OUTPUT_DIR = 'json_merge/classification_merge_output_v3'\n",
    "# merged 파일 전체를 대상으로 (분류, 요약, 질의응답 등)\n",
    "file_pattern = os.path.join(OUTPUT_DIR, 'merged_*_final.json')\n",
    "\n",
    "# 2) 파일 목록 수집\n",
    "files = glob.glob(file_pattern)\n",
    "\n",
    "# 3) 재귀 함수로 모든 'input' 키 제거\n",
    "def remove_input(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        # 최상위 'input' 삭제\n",
    "        obj.pop('input', None)\n",
    "        for v in obj.values():\n",
    "            remove_input(v)\n",
    "    elif isinstance(obj, list):\n",
    "        for item in obj:\n",
    "            remove_input(item)\n",
    "\n",
    "# 4) 각 파일 처리\n",
    "for fp in tqdm(files, desc='Removing duplicate inputs'):\n",
    "    with open(fp, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    remove_input(data)\n",
    "    with open(fp, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f'✅ Removed \"input\" fields from {len(files)} merged JSON files.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362c7d5b-2b4f-410a-9090-427a09423927",
   "metadata": {},
   "source": [
    "## 요약 Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4e005146-e895-4ccb-af1b-c1e0f843e5b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "요약 세션 병합: 100%|█████████████████████| 3600/3600 [00:02<00:00, 1248.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 요약 폴더 병합 완료 → json_merge/summary_merge_output_v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1) 입력·출력 폴더 설정\n",
    "INPUT_DIR  = 'data_v3/summary'           # 실제 요약 JSON이 있는 폴더\n",
    "OUTPUT_DIR = 'json_merge/summary_merge_output_v3'      # 병합 결과를 저장할 폴더\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# 2) session_id별로 파일 경로 모으기\n",
    "sessions = defaultdict(list)\n",
    "for fp in glob.glob(os.path.join(INPUT_DIR, '02_*_*.json')):\n",
    "    fname = os.path.basename(fp)\n",
    "    parts = fname.split('_')\n",
    "    # parts = ['02', '요약', '<session_id>', '1.json']\n",
    "    if parts[0] != '02':\n",
    "        continue\n",
    "    session_id = parts[2]\n",
    "    sessions[session_id].append(fp)\n",
    "\n",
    "# 3) 세션별 병합\n",
    "for session_id, fps in tqdm(sessions.items(), desc='요약 세션 병합'):\n",
    "    if not fps:\n",
    "        continue\n",
    "    \n",
    "    # 3-1) 번호 순서대로 정렬 (1 → 2 → 3 → 4…)\n",
    "    ordered = sorted(\n",
    "        fps,\n",
    "        key=lambda p: int(os.path.basename(p).split('_')[-1].split('.')[0])\n",
    "    )\n",
    "    \n",
    "    # 3-2) JSON 로드 & 같은 consulting_content 기준으로 data 병합\n",
    "    merged = {}\n",
    "    for p in ordered:\n",
    "        with open(p, 'r', encoding='utf-8') as f:\n",
    "            arr = json.load(f)\n",
    "        rec = arr[0] if isinstance(arr, list) else arr\n",
    "        \n",
    "        # 공통된 상담 대화 원문으로 그룹화\n",
    "        content = rec.get('consulting_content', '')\n",
    "        if content not in merged:\n",
    "            # instructions, input 제외한 기본 메타만 보관\n",
    "            base = {k: v for k, v in rec.items() if k not in ('instructions', 'input')}\n",
    "            merged[content] = {'base': base, 'data': []}\n",
    "        \n",
    "        # instructions[0]['data'] 병합\n",
    "        merged[content]['data'].extend(\n",
    "            rec['instructions'][0].get('data', [])\n",
    "        )\n",
    "    \n",
    "    # 3-3) 최종 리스트 생성\n",
    "    cleaned = []\n",
    "    for content, info in merged.items():\n",
    "        obj = info['base'].copy()\n",
    "        obj['consulting_content'] = content\n",
    "        obj['instructions'] = [{\n",
    "            'tuning_type': '요약',\n",
    "            'data': info['data']\n",
    "        }]\n",
    "        cleaned.append(obj)\n",
    "    \n",
    "    # 4) JSON으로 저장\n",
    "    out = {\n",
    "        'session_id': session_id,\n",
    "        '요약': cleaned\n",
    "    }\n",
    "    out_path = os.path.join(OUTPUT_DIR, f'merged_summary_{session_id}.json')\n",
    "    with open(out_path, 'w', encoding='utf-8') as fw:\n",
    "        json.dump(out, fw, ensure_ascii=False, indent=2)\n",
    "\n",
    "print('✅ 요약 폴더 병합 완료 →', OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "119ebff0-129c-40d9-bcd5-524e92699b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Removing inputs in summary: 100%|█████████| 3600/3600 [00:00<00:00, 5254.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 모든 merged_summary 파일에서 'input' 제거 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1) 병합된 요약 파일들이 있는 폴더\n",
    "INPUT_DIR = 'json_merge/summary_merge_output_v3'\n",
    "pattern   = os.path.join(INPUT_DIR, 'merged_summary_*.json')\n",
    "\n",
    "# 2) 각 파일마다 'input' 키 제거\n",
    "for filepath in tqdm(glob.glob(pattern), desc='Removing inputs in summary'):\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # '요약' 리스트 내부 각 레코드의 'input' 삭제\n",
    "    for rec in data.get('요약', []):\n",
    "        rec.pop('input', None)\n",
    "        # instructions.data 내부의 'input'도 삭제\n",
    "        for inst in rec.get('instructions', []):\n",
    "            for item in inst.get('data', []):\n",
    "                item.pop('input', None)\n",
    "\n",
    "    # 3) 변경 내용 덮어쓰기 저장\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"✅ 모든 merged_summary 파일에서 'input' 제거 완료\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad3bcaf-f59c-4371-b05e-b2b89398f146",
   "metadata": {},
   "source": [
    "## 질의응답"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "73f9f162-383f-499e-951a-08bf44d7db7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "질의응답 세션 병합: 100%|█████████████████| 3533/3533 [00:02<00:00, 1761.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 질의응답 폴더 병합 완료 → json_merge/qa_merge_output_v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Removing inputs in QnA: 100%|█████████████| 3533/3533 [00:00<00:00, 6267.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ QnA merged 파일에서 'input' 제거 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1) 입력·출력 폴더 설정\n",
    "INPUT_DIR  = 'data_v3/qa'   # 질의응답 JSON 파일들이 있는 폴더\n",
    "OUTPUT_DIR = 'json_merge/qa_merge_output_v3'      # 병합 결과를 저장할 폴더\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# 2) session_id별로 파일 경로 모으기\n",
    "sessions = defaultdict(list)\n",
    "for fp in glob.glob(os.path.join(INPUT_DIR, '03_*_*.json')):\n",
    "    fname = os.path.basename(fp)\n",
    "    parts = fname.split('_')\n",
    "    # 파일명 예시: '03_질의응답_44017_1.json'\n",
    "    if parts[0] != '03':\n",
    "        continue\n",
    "    session_id = parts[2]\n",
    "    sessions[session_id].append(fp)\n",
    "\n",
    "# 3) 세션별 병합\n",
    "for session_id, fps in tqdm(sessions.items(), desc='질의응답 세션 병합'):\n",
    "    if not fps:\n",
    "        continue\n",
    "    \n",
    "    # 3-1) 번호 순서대로 정렬 (1 → 2 → 3 → …)\n",
    "    ordered = sorted(\n",
    "        fps,\n",
    "        key=lambda p: int(os.path.basename(p).split('_')[-1].split('.')[0])\n",
    "    )\n",
    "    \n",
    "    # 3-2) JSON 로드 & 같은 consulting_content 기준으로 data 병합\n",
    "    merged = {}\n",
    "    for p in ordered:\n",
    "        with open(p, 'r', encoding='utf-8') as f:\n",
    "            arr = json.load(f)\n",
    "        rec = arr[0] if isinstance(arr, list) else arr\n",
    "        content = rec.get('consulting_content', '')\n",
    "        \n",
    "        if content not in merged:\n",
    "            # instructions/input 제외한 메타데이터만 저장\n",
    "            base = {k: v for k, v in rec.items() if k not in ('instructions', 'input')}\n",
    "            merged[content] = {'base': base, 'data': []}\n",
    "        \n",
    "        # instructions[0]['data'] 병합\n",
    "        merged[content]['data'].extend(\n",
    "            rec['instructions'][0].get('data', [])\n",
    "        )\n",
    "    \n",
    "    # 3-3) 최종 오브젝트 리스트 생성\n",
    "    cleaned = []\n",
    "    for content, info in merged.items():\n",
    "        obj = info['base'].copy()\n",
    "        obj['consulting_content'] = content\n",
    "        obj['instructions'] = [{\n",
    "            'tuning_type': '질의응답',\n",
    "            'data': info['data']\n",
    "        }]\n",
    "        cleaned.append(obj)\n",
    "    \n",
    "    # 4) JSON으로 저장\n",
    "    out = {\n",
    "        'session_id': session_id,\n",
    "        '질의응답': cleaned\n",
    "    }\n",
    "    out_path = os.path.join(OUTPUT_DIR, f'merged_qna_{session_id}.json')\n",
    "    with open(out_path, 'w', encoding='utf-8') as fw:\n",
    "        json.dump(out, fw, ensure_ascii=False, indent=2)\n",
    "\n",
    "print('✅ 질의응답 폴더 병합 완료 →', OUTPUT_DIR)\n",
    "\n",
    "# 5) 병합된 파일에서 중복된 'input' 필드 제거\n",
    "pattern = os.path.join(OUTPUT_DIR, 'merged_qna_*.json')\n",
    "for filepath in tqdm(glob.glob(pattern), desc='Removing inputs in QnA'):\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    # 최상위 'input' 삭제\n",
    "    data.pop('input', None)\n",
    "    for rec in data.get('질의응답', []):\n",
    "        rec.pop('input', None)\n",
    "        for inst in rec.get('instructions', []):\n",
    "            for item in inst.get('data', []):\n",
    "                item.pop('input', None)\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"✅ QnA merged 파일에서 'input' 제거 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6ef11b-3d9e-42e5-8f44-d71764103ac7",
   "metadata": {},
   "source": [
    "# 최종 Merge (분류 + 요약 + 질의응답)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5a30dce1-b4a3-4f0f-9fcf-f26698349960",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "최종 세션 통합: 100%|█████████████████████| 3600/3600 [00:01<00:00, 1867.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 최종 통합 완료 → json_merge/integration_data_v3/final_merged_<session_id>.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1) 폴더 경로 설정\n",
    "CLASS_DIR   = 'json_merge/classification_merge_output_v3' # 분류 파일 보관 폴더\n",
    "SUMMARY_DIR = 'json_merge/summary_merge_output_v3'        # 요약 파일 보관 폴더\n",
    "QNA_DIR     = 'json_merge/qa_merge_output_v3'            # 질의응답 파일 보관 폴더\n",
    "FINAL_DIR   = 'json_merge/integration_data_v3'          # 최종 머지 파일 저장 폴더\n",
    "os.makedirs(FINAL_DIR, exist_ok=True)\n",
    "\n",
    "# 2) 세션ID별로 파일 경로 매핑\n",
    "sessions = defaultdict(dict)\n",
    "for path in glob.glob(os.path.join(CLASS_DIR, 'merged_classification_*_final.json')):\n",
    "    sid = os.path.basename(path).split('_')[2]\n",
    "    sessions[sid]['class'] = path\n",
    "for path in glob.glob(os.path.join(SUMMARY_DIR, 'merged_summary_*.json')):\n",
    "    sid = os.path.basename(path).split('_')[2].split('.')[0]\n",
    "    sessions[sid]['summary'] = path\n",
    "for path in glob.glob(os.path.join(QNA_DIR, 'merged_qna_*.json')):\n",
    "    sid = os.path.basename(path).split('_')[2].split('.')[0]\n",
    "    sessions[sid]['qna'] = path\n",
    "\n",
    "# 3) 세션별로 합치기\n",
    "for sid, files in tqdm(sessions.items(), desc='최종 세션 통합'):\n",
    "    if not all(k in files for k in ('class','summary','qna')):\n",
    "        continue  # 세 가지 모두 있어야 처리\n",
    "\n",
    "    # 3-1) 각각 로드\n",
    "    cls = json.load(open(files['class'],   'r', encoding='utf-8'))\n",
    "    smm = json.load(open(files['summary'], 'r', encoding='utf-8'))\n",
    "    qna = json.load(open(files['qna'],     'r', encoding='utf-8'))\n",
    "\n",
    "    # 3-2) 공통 대화 원문 추출\n",
    "    content = cls['분류'][0]['consulting_content']\n",
    "\n",
    "    # 3-3) 모든 instructions.data 항목 모으기\n",
    "    all_inst = []\n",
    "    for rec in cls['분류']:\n",
    "        for blk in rec['instructions']:\n",
    "            all_inst.extend(blk.get('data', []))\n",
    "    for rec in smm['요약']:\n",
    "        for blk in rec['instructions']:\n",
    "            all_inst.extend(blk.get('data', []))\n",
    "    for rec in qna['질의응답']:\n",
    "        for blk in rec['instructions']:\n",
    "            all_inst.extend(blk.get('data', []))\n",
    "\n",
    "    # 3-4) instruction_id 오름차순 정렬 + 중복 제거\n",
    "    all_inst_sorted = sorted(all_inst, key=lambda x: x.get('instruction_id', 0))\n",
    "    seen = set()\n",
    "    dedup = []\n",
    "    for inst in all_inst_sorted:\n",
    "        iid = inst.get('instruction_id')\n",
    "        if iid not in seen:\n",
    "            seen.add(iid)\n",
    "            dedup.append(inst)\n",
    "\n",
    "    # 3-5) 최종 구조 생성\n",
    "    final = {\n",
    "        'session_id': sid,\n",
    "        'consulting_content': content,\n",
    "        'instructions': dedup\n",
    "    }\n",
    "\n",
    "    # 4) 저장\n",
    "    out_path = os.path.join(FINAL_DIR, f'final_merged_{sid}.json')\n",
    "    with open(out_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(final, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f'✅ 최종 통합 완료 → {FINAL_DIR}/final_merged_<session_id>.json')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3ab84212-4653-43b7-83ea-b99c5f76cd21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging classification meta into final: 100%|█| 3600/3600 [00:00<00:00, 3652.77i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Classification 메타가 추가된 final 파일 생성 완료.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1) 디렉토리 설정\n",
    "CLASS_DIR      = 'json_merge/classification_merge_output_v3'   # merged_classification_XXX_final.json 들이 있는 폴더\n",
    "FINAL_DIR      = 'json_merge/integration_data_v3'            # 지금까지 만든 final_merged_XXX.json 들이 있는 폴더\n",
    "OUTPUT_DIR     = 'json_merge/integration_data_v3_with_meta'  # 메타 정보가 추가된 최종 파일들을 저장할 폴더\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# 2) 모든 classification 파일 순회\n",
    "class_files = glob.glob(os.path.join(CLASS_DIR, 'merged_classification_*_final.json'))\n",
    "for class_fp in tqdm(class_files, desc='Merging classification meta into final'):\n",
    "    # 세션 ID 추출 (파일명 끝에서 두 번째 언더바 사이)\n",
    "    session_id = os.path.basename(class_fp).split('_')[-2]\n",
    "    \n",
    "    # 3) classification 메타 불러오기\n",
    "    class_data = json.load(open(class_fp, 'r', encoding='utf-8'))\n",
    "    # 분류 리스트의 첫 번째 원소에 담긴 메타 정보만 추출\n",
    "    meta_item = class_data['분류'][0]\n",
    "    meta_fields = {\n",
    "        'source':               meta_item.get('source'),\n",
    "        'source_id':            meta_item.get('source_id'),\n",
    "        'consulting_category':  meta_item.get('consulting_category'),\n",
    "        'consulting_time':      meta_item.get('consulting_time'),\n",
    "        'consulting_turns':     meta_item.get('consulting_turns'),\n",
    "        'consulting_length':    meta_item.get('consulting_length'),\n",
    "    }\n",
    "    \n",
    "    # 4) 기존 final 파일 불러오기\n",
    "    final_fp = os.path.join(FINAL_DIR, f'final_merged_{session_id}.json')\n",
    "    if not os.path.exists(final_fp):\n",
    "        # final 파일이 없으면 건너뜀\n",
    "        continue\n",
    "    final_data = json.load(open(final_fp, 'r', encoding='utf-8'))\n",
    "    \n",
    "    # 5) 메타를 최상단으로 병합\n",
    "    # 순서를 보장하려면 새로운 dict 생성\n",
    "    new_final = {}\n",
    "    new_final['session_id'] = final_data.get('session_id')\n",
    "    # classification 메타 삽입\n",
    "    new_final.update(meta_fields)\n",
    "    # 나머지 기존 final 데이터 삽입\n",
    "    # (consulting_content, instructions 등)\n",
    "    for k, v in final_data.items():\n",
    "        if k == 'session_id':\n",
    "            continue\n",
    "        new_final[k] = v\n",
    "    \n",
    "    # 6) 저장\n",
    "    out_fp = os.path.join(OUTPUT_DIR, f'final_merged_{session_id}_with_meta.json')\n",
    "    with open(out_fp, 'w', encoding='utf-8') as fw:\n",
    "        json.dump(new_final, fw, ensure_ascii=False, indent=2)\n",
    "\n",
    "print('✅ Classification 메타가 추가된 final 파일 생성 완료.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84947b38-e410-4607-b702-03bb5e788034",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
